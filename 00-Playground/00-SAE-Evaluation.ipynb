{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     import google.colab # type: ignore\n",
    "#     from google.colab import output\n",
    "#     COLAB = True\n",
    "#     %pip install sae-lens transformer-lens\n",
    "# except:\n",
    "#     COLAB = False\n",
    "from IPython import get_ipython # type: ignore\n",
    "ipython = get_ipython(); assert ipython is not None\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Device setup\n",
    "GPU_TO_USE = 2\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = f\"cuda:{GPU_TO_USE}\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# utility to clear variables out of the memory & and clearing cuda cache\n",
    "import gc\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['layer_9/width_16k/canonical',\n",
       "  'layer_20/width_16k/canonical',\n",
       "  'layer_31/width_16k/canonical'],\n",
       " ['layer_9/width_16k/canonical',\n",
       "  'layer_20/width_16k/canonical',\n",
       "  'layer_31/width_16k/canonical'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL = \"gemma-2-9b\"\n",
    "INSTRUCT_MODEL = \"gemma-2-9b-it\"\n",
    "\n",
    "SAE_BASE_RELEASE = 'gemma-scope-9b-pt-res'\n",
    "SAE_INSTRUCT_RELEASE = 'gemma-scope-9b-it-res'\n",
    "\n",
    "USE_CANONICAL = True\n",
    "if USE_CANONICAL:\n",
    "    SAE_BASE_RELEASE = SAE_BASE_RELEASE + '-canonical'\n",
    "    SAE_INSTRUCT_RELEASE = SAE_INSTRUCT_RELEASE + '-canonical'\n",
    "\n",
    "SAE_INSTRUCT_LAYERS = [9, 20, 31]\n",
    "SAE_WIDTH = '16k'\n",
    "SAE_DTYPE = torch.bfloat16\n",
    "\n",
    "# Specify L0 values when using non-canonical SAEs\n",
    "base_sae_l0_values = {\n",
    "    '16k': [51, 57, 63],\n",
    "    # '131k': []\n",
    "}\n",
    "\n",
    "instruct_sae_l0_values = {\n",
    "    '16k': [47, 47, 76],\n",
    "    # '131k': []\n",
    "}\n",
    "\n",
    "if USE_CANONICAL:\n",
    "    base_sae_ids = [f'layer_{layer}/width_{SAE_WIDTH}/canonical' for layer in SAE_INSTRUCT_LAYERS]\n",
    "    instruct_sae_ids = base_sae_ids[:]\n",
    "else:\n",
    "    base_sae_ids = [f'layer_{layer}/width_{SAE_WIDTH}/average_l0_{l0}' for layer, l0 in zip(SAE_INSTRUCT_LAYERS, base_sae_l0_values[SAE_WIDTH])]\n",
    "    instruct_sae_ids = [f'layer_{layer}/width_{SAE_WIDTH}/average_l0_{l0}' for layer, l0 in zip(SAE_INSTRUCT_LAYERS, instruct_sae_l0_values[SAE_WIDTH])]\n",
    "\n",
    "base_sae_ids, instruct_sae_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH_STR = '../data'\n",
    "\n",
    "datapath = Path(DATAPATH_STR)\n",
    "datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Experiment(Enum):\n",
    "    SUBSTITUTION_LOSS = 'SubstitutionLoss'\n",
    "    L0_LOSS = 'L0_loss'\n",
    "\n",
    "TOKENS_SAMPLE = {\n",
    "    Experiment.SUBSTITUTION_LOSS: [],\n",
    "    Experiment.L0_LOSS: [],\n",
    "}\n",
    "\n",
    "TOTAL_BATCHES = {\n",
    "    Experiment.SUBSTITUTION_LOSS: 50,\n",
    "    Experiment.L0_LOSS: 50,\n",
    "}\n",
    "\n",
    "def get_batch_size(key: Experiment):\n",
    "    return TOTAL_BATCHES[key]\n",
    "\n",
    "def get_tokens_sample(key: Experiment):\n",
    "    return TOKENS_SAMPLE[key]\n",
    "\n",
    "def set_tokens_sample(key: Experiment, token_sample):\n",
    "    TOKENS_SAMPLE[key] = token_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L0_loss(x, threshold=1e-8):\n",
    "    \"\"\"\n",
    "    Expects a tensor x of shape [N_TOKENS, N_SAE].\n",
    "\n",
    "    Returns a scalar representing the mean value of activated features (i.e. values across the N_SAE dimensions bigger than\n",
    "    the threshhold), a.k.a. L0 loss.\n",
    "    \"\"\"\n",
    "    return (x > threshold).float().sum(-1).mean()\n",
    "\n",
    "def compute_substitution_loss(tokens, tokens_mask, model, sae, sae_layer, reconstruction_metric=None):\n",
    "    '''\n",
    "    Expects a tensor of input tokens of shape [N_BATCHES, N_CONTEXT] and a tokens_mask\n",
    "    of shape [N_BATCHES, N_CONTEXT] where 1 indicates positions to exclude from substitution.\n",
    "\n",
    "    Returns two losses:\n",
    "    1. Clean loss - loss of the normal forward pass of the model at the input tokens.\n",
    "    2. Substitution loss - loss when substituting SAE reconstructions of the residual stream at the SAE layer.\n",
    "    '''\n",
    "    assert tokens_mask.shape == tokens.shape, \"tokens_mask shape must match tokens shape.\"\n",
    "\n",
    "    # Run the model with cache to get the original activations and clean loss\n",
    "    loss_clean, cache = model.run_with_cache(tokens, names_filter=[sae_layer], return_type=\"loss\")\n",
    "\n",
    "    # Fetch and detach the original activations\n",
    "    original_activations = cache[sae_layer]  # shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "    # Flatten the tokens_mask to match activations shape: [batch_size * seq_len]\n",
    "    tokens_mask_flat = tokens_mask.view(-1)\n",
    "\n",
    "    # Flatten activations: [batch_size * seq_len, d_model]\n",
    "    activations_flat = original_activations.view(-1, original_activations.shape[-1])\n",
    "\n",
    "    # Filter activations using the inverse of tokens_mask (i.e., positions where mask is 0 are kept for substitution)\n",
    "    valid_activations = activations_flat[tokens_mask_flat == 0]\n",
    "\n",
    "    # Get the SAE reconstructed activations\n",
    "    post_reconstructed = sae.forward(valid_activations)  # shape: [valid_activations, d_model]\n",
    "\n",
    "    # Update the reconstruction quality metric, if provided\n",
    "    if reconstruction_metric:\n",
    "        reconstruction_metric.update(post_reconstructed.flatten().float(), valid_activations.flatten().float())\n",
    "\n",
    "    # Free unused variables early to save memory\n",
    "    del original_activations, valid_activations, cache\n",
    "    clear_cache()\n",
    "\n",
    "    # Modified hook function using tokens_mask\n",
    "    def hook_function(activations, hook, new_activations, tokens_mask):\n",
    "        # activations: [batch_size, seq_len, d_model]\n",
    "        # new_activations: [valid_activations, d_model]\n",
    "        # tokens_mask: [batch_size, seq_len]\n",
    "\n",
    "        # Flatten activations and mask\n",
    "        activations_flat = activations.view(-1, activations.shape[-1])\n",
    "        tokens_mask_flat = tokens_mask.view(-1)\n",
    "\n",
    "        # Replace activations where tokens_mask is 0 (i.e., not masked)\n",
    "        activations_flat[tokens_mask_flat == 0] = new_activations\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        activations = activations_flat.view(activations.shape)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    # Run the model again with hooks to substitute activations at the SAE layer using tokens_mask\n",
    "    loss_reconstructed = model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae_layer, partial(hook_function, new_activations=post_reconstructed, tokens_mask=tokens_mask))]\n",
    "    )\n",
    "\n",
    "    # Clean up the reconstructed activations and clear memory\n",
    "    del post_reconstructed\n",
    "    clear_cache()\n",
    "\n",
    "    return loss_clean, loss_reconstructed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8fba4763b49c08fae0fff2b94b48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-9b-it into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedSAETransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-41): 42 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln1_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(INSTRUCT_MODEL, device=device, dtype=torch.bfloat16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccec744f6fe472da031059b2f3d1bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed6c4495e0a410ebc9156cb5bd2dbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SAEConfig(architecture='jumprelu', d_in=3584, d_sae=16384, activation_fn_str='relu', apply_b_dec_to_input=False, finetuning_scaling_factor=False, context_size=1024, model_name='gemma-2-9b', hook_name='blocks.9.hook_resid_post', hook_layer=9, hook_head_index=None, prepend_bos=True, dataset_path='monology/pile-uncopyrighted', dataset_trust_remote_code=True, normalize_activations=None, dtype='torch.bfloat16', device='cuda:2', sae_lens_training_version=None, activation_fn_kwargs={}, neuronpedia_id='gemma-2-9b/9-gemmascope-res-16k', model_from_pretrained_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_saes = [SAE.from_pretrained(\n",
    "                release = SAE_BASE_RELEASE,\n",
    "                sae_id = sae_id,\n",
    "                device = device\n",
    "            )[0].to(SAE_DTYPE) for sae_id in base_sae_ids]\n",
    "base_saes[0].cfg.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PT SAEs on the Instruct model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution/Delta loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
